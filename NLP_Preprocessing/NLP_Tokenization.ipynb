{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fd14fb",
   "metadata": {},
   "source": [
    "# NLP_preprocessing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22e57f",
   "metadata": {},
   "source": [
    "- In natural language processing (NLP), tokenization refers to the process of breaking down a text or a sequence of words into smaller units called tokens. These tokens can be individual words, sentences, or even characters, depending on the level of granularity required for the analysis.\n",
    "\n",
    "- Tokenization is a crucial step in NLP tasks as it serves as the foundation for various downstream processes such as part-of-speech tagging, named entity recognition, sentiment analysis, and machine translation. By breaking down the text into tokens, it becomes easier to analyze and understand the linguistic structure and meaning of the text.\n",
    "\n",
    "- Tokenization methods can vary based on the specific requirements of the task or the language being processed. Common tokenization techniques include whitespace tokenization, where tokens are split based on spaces, and word-level tokenization, where tokens are individual words. More advanced tokenization techniques also exist, such as subword tokenization, which breaks down words into subword units, and character-level tokenization, where tokens are individual characters.\n",
    "\n",
    "- Overall, tokenization is a fundamental process in NLP that enables the effective analysis and processing of textual data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91428efb",
   "metadata": {},
   "source": [
    "### Input Text: \"I love natural language processing!\"\n",
    "\n",
    "- Tokenization Output: [\"I\", \"love\", \"natural\", \"language\", \"processing\", \"!\"]\n",
    "\n",
    "- In this example, the input text is tokenized into individual words. Each word becomes a separate token, and the punctuation mark (\"!\") is also treated as a separate token.\n",
    "\n",
    "- Tokenization breaks down the input text into meaningful units, allowing further analysis and processing of the text at the word level.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e7920",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f896850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74fa2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "197e62eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love natural language processing!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde22377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love natural language processing!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token = nltk.sent_tokenize(text)\n",
    "sent_token # this is a sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a100d69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'natural', 'language', 'processing', '!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = nltk.word_tokenize(text)\n",
    "word_token # this is a word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7aa92",
   "metadata": {},
   "source": [
    "### Let's take big text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe10f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_text = \"Natural language processing (NLP) is a field of study that focuses on the interaction between computers and human language. It involves tasks such as text classification, sentiment analysis, machine translation, and named entity recognition. Tokenization is a fundamental step in NLP, where the input text is divided into smaller units called tokens. These tokens can be individual words, phrases, or even characters. The tokenization process serves as the foundation for various NLP tasks and allows for effective analysis and processing of textual data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b96af37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural language processing (NLP) is a field of study that focuses on the interaction between computers and human language.',\n",
       " 'It involves tasks such as text classification, sentiment analysis, machine translation, and named entity recognition.',\n",
       " 'Tokenization is a fundamental step in NLP, where the input text is divided into smaller units called tokens.',\n",
       " 'These tokens can be individual words, phrases, or even characters.',\n",
       " 'The tokenization process serves as the foundation for various NLP tasks and allows for effective analysis and processing of textual data.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token = nltk.sent_tokenize(bg_text)\n",
    "sent_token # this is a sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e981184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a field of study that focuses on the interaction between computers and human language.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "668707f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'study',\n",
       " 'that',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " '.',\n",
       " 'It',\n",
       " 'involves',\n",
       " 'tasks',\n",
       " 'such',\n",
       " 'as',\n",
       " 'text',\n",
       " 'classification',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'machine',\n",
       " 'translation',\n",
       " ',',\n",
       " 'and',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fundamental',\n",
       " 'step',\n",
       " 'in',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'where',\n",
       " 'the',\n",
       " 'input',\n",
       " 'text',\n",
       " 'is',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'units',\n",
       " 'called',\n",
       " 'tokens',\n",
       " '.',\n",
       " 'These',\n",
       " 'tokens',\n",
       " 'can',\n",
       " 'be',\n",
       " 'individual',\n",
       " 'words',\n",
       " ',',\n",
       " 'phrases',\n",
       " ',',\n",
       " 'or',\n",
       " 'even',\n",
       " 'characters',\n",
       " '.',\n",
       " 'The',\n",
       " 'tokenization',\n",
       " 'process',\n",
       " 'serves',\n",
       " 'as',\n",
       " 'the',\n",
       " 'foundation',\n",
       " 'for',\n",
       " 'various',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'and',\n",
       " 'allows',\n",
       " 'for',\n",
       " 'effective',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'processing',\n",
       " 'of',\n",
       " 'textual',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = nltk.word_tokenize(bg_text)\n",
    "word_token # this is a word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb550bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
